<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="robots" content="index, follow" />
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Jinlong Li, 李金龙, Computer Vision, Machine Learning, Reinforcement Learning, Deep Learning">

<link rel="stylesheet" href="./jemdoc.css" type="text/css" />
<script src="jquery.min.js"></script>
<!-- <link rel="shortcut icon" href="./files/csu.ico"> -->
<link rel="shortcut icon" href="./figure/jinlong_logo1.png">

<title>Jinlong Li(李金龙)</title>

</head>
 
 
<body>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<table class="imgtable"><tr><td>
<a><img src="./files/Jinlong_profile.png" alt="" height="250px" /></a>&nbsp;</td>
<td align="left"><p><font size="5">Jinlong Li (</font><font size="5"; font style="font-family:Microsoft YaHei">李金龙</font><font size="5">)   </font></a> <br /><br>

    <i> <b> CS PhD &vert; Computer Vision  &vert; Deep Learning &vert; Autonomous Driving</b> </i><br /><br>

<!-- <a href="https://www.csuohio.edu/" target="_blank"> in Cleveland State University</a><br /> -->
<!-- Email: lijinlong1117@gmail.com (prior); &nbsp;&nbsp;&nbsp;&nbsp; j.li56@vikes.csuohio.edu <br /> -->

Email: lijinlong1117@gmail.com /  jinlong@tamu.edu <br />
<br />
[<a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank">Google Scholar</a>] 
[<a href="https://github.com/jinlong17" target="_blank">GitHub</a>] 
[<a href="https://www.researchgate.net/profile/Jinlong-Li-22" target="_blank">ResearchGate</a>] 
[<a href="https://orcid.org/0000-0002-7784-8363" target="_blank">ORCID</a>]
[<a href="https://www.notion.so/Jinlong-s-HOME-8c2927563ebc4537bb4c862f82ddbc95" target="_blank">Notion</a>] 
<!-- [<a href="./files/QQ.png" target="_blank">QQ</a>]  -->
<!-- [<a href="./files/weixin.png" target="_blank"><font style="font-family:Microsoft YaHei">微信</font></a>]  -->
[<a href="./files/jinlong_CV_2023fall.pdf" target="_blank"><font style="font-family:Microsoft YaHei">Resume</font></a>]
<br />
<br />
<br />
<!-- Address: FH 311, Department of CS, Cleveland State University, Cleveland, OH 44115, USA<br /> -->
<class="staffshortcut">
 <A HREF="#News">News</A> | 
 <A HREF="#Selected Works">Selected Works</A> | 
 <A HREF="#About Me">About Me</A> | 
 <A HREF="#Education and Intern">Education and Intern</A> | 
 <A HREF="#Publications">Publications</A> | 
 <!-- <A HREF="#Projects">Projects</A> |  -->
 <A HREF="#Services">Services</A> 
 <!-- <A HREF="#Awards">Awards</A> -->

</td></tr></table>

 
<A NAME="About Me"><h2>About Me</h2></A>
Focusing on the innovative and practical solutions for real-world interdisciplinary researches by combining different methods. Currently, I focus on the following research topics:
<ul>
<li>Object detection using transfer learning in Autonomous driving and Intelligent transportation system.</li>
 <li>3D detection for cooperative perception in autonomous vehicles.</li>
</ul>
<br />


<A NAME="News"><h2>News</h2></A>
<ul>
<li> <b> [2024.08] </b> Jinlong started his position as a Postdoctoral Researcher in the TACO Group at Texas A&M University.<a href="https://taco-group.github.io/group.html" target="_blank">[TACO-Group]</a></li>
<li> <b> [2024.05] </b> Jinlong began his research internship with the AI Visual Team at OPPO US Research Center in Palo Alto, CA.</li>
<li> <b> [2024.04] </b> Jinlong successfully passed his Computer Science Ph.D. Dissertation Defense April 11, 2024.</li>
<li> <b> [2024.04] </b> Jinlong received the CSU Outstanding Achievement Award for Doctoral Research. </li>
<li> <b> [2024.02] </b> <b> One </b> paper has been accepted by <b> CVPR 2024 </b>!  <a href="https://jinlong17.github.io/LightDiff/" target="_blank"> [Project page]</a></li>
<li> <b> [2024.02] </b> <b> Three </b> paper has been accepted by <b> ICRA 2024 </b>! </li>
<li> <b> [2023.05] </b> Jinlong successfully passed the Computer Science Ph.D. Proposal presentation on May 01, 2023. Congratulations.</li>
<li> <b> [2023.03] </b> One paper has been accepted by IEEE Transactions on Intelligent Vehicles.</li>
<li> <b> [2023.02] </b> One paper is accepted at CVPR 2023, it is selected as <b>Highlight (2.5% of 9155 submissions)</b></li>
<li> <b> [2023.02] </b> Jinlong successfully passed the CS Ph.D. Qualifying Exam on Feb. 02 2023. Now he is a Ph.D. Candidate in Department of Computer Science, Cleveland State University.</li>
<li> <b> [2022.10] </b> Two papers (Colorizing Old Photos; Domain Adaptive Object Detection) have been accepted by WACV 2023.</li>
 <li> <b> [2021.08] </b> Jinlong successfully arrived at Cleveland, Ohio, USA. Now he is a Ph.D. student in Department of Computer Science, Cleveland State University.</li>
<li> <b> [2021.06] </b> Jinlong  successfully awarded his M.E. degree of Computer Science in Chang'an University in China.</li>
<li> <b> [2021.01] </b> One paper about Domain Adaptation based Vehicle Detection is accepted by Transportation Research Part C.[<a href= "https://www.sciencedirect.com/science/article/abs/pii/S0968090X20308433" target="_blank">Link</a>]</li>
</ul>
<br />



<A NAME="Education and Intern"><h2>Education and Experience</h2></A>
<!-- <li>2021.09-2024.08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D in <a href="http://www.lmars.whu.edu.cn/" target="_blank"> EECS</a>, <a href="https://www.csuohio.edu/" target="_blank">Cleveland State University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://cis.csuohio.edu/~h.yu/me.html" target="_blank">Hongkai Yu</a></li> -->

<!-- <li>2018.09-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M.E. in <a href="http://main.sgg.whu.edu.cn/" target="_blank">Computer Science and Technology</a>, <a href="http://www.xahu.edu.cn/" target="_blank">Chang'an University</a>. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://js.chd.edu.cn/xxgcxy/xzg/list.htm" target="_blank">Zhigang Xu</a></li> -->
<ul>


<li>2024.08-present &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Postdoctoral Researcher in TACO-Group @ Texas A&M University.<a href="https://taco-group.github.io/group.html" target="_blank"> (TACO-Group)</a>

<li>2024.05-2024.08 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research intern in AI-Visual Enhancement and Understanding Team, OPPO US Research Center.

<li>2021.09-2024.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ph.D. in Engineering (Computer Science Track), Cleveland State University.

<li>2018.09-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; M.E. in Computer Science and Technology, Chang'an University.

<li>2014.09-2018.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B.E. in Road Bridge and River Crossing Engineering (Excellent Engineer), Chang'an University.

</ul>
<br />



</section>
<div class="row">
    <A NAME="Selected Works"><h2>Selected Works</h2></A>




        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/Co_pipeline.png" style="border:1px solid black" alt="" height="165px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">
                    CoMamba: Real-time Cooperative Perception Unlocked with State Space Models <a href="https://jinlong17.github.io/CoMamba/" target="_blank"> [Project Page]</a></h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://jinlong17.github.io/" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                <a href="https://jiachenli94.github.io/" target="_blank">Jiachen Li</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a> ,
                <a href="https://vztu.github.io/" target="_blank">Zhengzhong Tu</a>
                
                <br> 
                <b>Preprint 2024 </b>
                <a href="https://arxiv.org/abs/2409.10699" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/CoMamba" target="_blank">[Code]</a>
                <!-- <a href="https://jinlong17.github.io/LightDiff/" target="_blank"> [Project Page]</a> -->
                <br>
                "CoMamba scales remarkably well, achieving linear-complexity costs in GFLOPs, latency, and GPU memory relative to the number of agents, while still maintaining excellent performance."

                 </b>
            </div>
            </div>
        </div>


        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/cvpr2024_lightdiff.png" style="border:1px solid black" alt="" height="165px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving  <a href="https://jinlong17.github.io/LightDiff/" target="_blank"> [Project Page]</a></h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?user=9ajdZaEAAAAJ&hl=en" target="_blank">Zhengzhong Tu</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://tsingqguo.github.io/" target="_blank">Qing Guo</a>,
                <a href="https://xujuefei.com/" target="_blank">Felix Juefei-Xu</a>,
                <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a> 
                <br> 
                <b>CVPR 2024 </b>
                <a href="https://arxiv.org/abs/2404.04804" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/LightDiff" target="_blank">[Code]</a>
                <!-- <a href="https://jinlong17.github.io/LightDiff/" target="_blank"> [Project Page]</a> -->
                <br>
                "We propose the Lighting Diffusion to enhance low-light camera images, mitigating the need for extensive nighttime data collection and preserving daytime performance."

                 </b>
            </div>
            </div>
        </div>




        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/TIV_DA.png" style="border:1px solid black" alt="" height="140px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather</h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://scholar.google.com/citations?user=UUPDTacAAAAJ&hl=en" target="_blank">Jin Ma</a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?user=dJ8izFAAAAAJ&hl=en" target="_blank">Qin Zou</a>,
                <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a> 
                <br> 
                <b>TIV 2024 </b>
                <a href="https://arxiv.org/abs/2307.09676" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/DA-Detect" target="_blank">[Code]</a>
                <br>
                "This work presents a domain adaptive object detection framework, which is specifically designed for intelligent vehicle perception in foggy and rainy weather conditions."

                 </b>
            </div>
            </div>
        </div>



        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/crossdata.png" style="border:1px solid black" alt="" height="155px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources</h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>
                <br> 
                <b>ICRA 2024 </b>
                <a href="https://arxiv.org/abs/2402.04273" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/Cross-Domain_V2V" target="_blank">[Code]</a>
                <br>
                "This work proposes the first research on multi-agent perception to address the Distribution Gap of different independent private data for training distinct agents."

                 </b>
            </div>
            </div>
        </div>
        


        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/advGPS.png" style="border:1px solid black" alt="" height="145px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">AdvGPS: Adversarial GPS for Multi-Agent Perception Attack</h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://scholar.google.com/citations?user=hr8eDYsAAAAJ&hl=en" target="_blank">Jianwu Fang</a>,
                <a href="https://xujuefei.com/" target="_blank">Felix Juefei-Xu</a>,
                <a href="https://tsingqguo.github.io/" target="_blank">Qing Guo</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>
                <br> 
                <b>ICRA 2024 </b>
                <a href="https://arxiv.org/abs/2401.17499" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/AdvGPS" target="_blank">[Code]</a>
                <br>
                "This work proposes the first research of adversarial GPS signals which are also stealthy for the V2V cooperative perception attacks."

                 </b>
            </div>
            </div>
        </div>




        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>

        <div style="display:flex; align-items:center;">
            <div style="flex:1; padding-right: 10px;">
            <img class="img-fluid img-rounded" src="./figure/S2R.png" style="border:1px solid black" alt="" height="165px", width="325px">
            </div>
            <div style="flex:2;">
            <div style="text-align:left; margin-bottom:10px;">
                <h3 style="text-align:left;color:black;">S2R-ViT for multi-agent cooperative perception: Bridging the gap from simulation to reality</h3>
            </div>
            <div style="line-height: 1.5;">
                <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                <a href="https://scholar.google.com/citations?hl=en&user=fGK5P7IAAAAJ" target="_blank">Xinyu Liu</a>,
                <a href="https://scholar.google.com/citations?user=d94_GW4AAAAJ&hl=en" target="_blank">Baolu Li</a>,
                <a href="https://scholar.google.com/citations?user=dJ8izFAAAAAJ&hl=en" target="_blank">Qin Zou</a>,
                <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>,
                <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>
                <br> 
                <b>ICRA 2024 </b>
                <a href="https://arxiv.org/abs/2307.07935" target="_blank"> [Paper]</a>
                <a href="https://github.com/jinlong17/S2R-ViT" target="_blank">[Code]</a>
                <br>
                "This work proposes the first Simulation-to-Reality transfer learning framework for multi-agent cooperative perception."

                 </b>
            </div>
            </div>
        </div>

        <div style="height: 20px;"></div>
        <div style="height: 20px;"></div>
        
        
            <div style="display:flex; align-items:center;">
                <div style="flex:1; padding-right: 10px;">
                <img class="img-fluid img-rounded" src="./figure/LC.png" style="border:1px solid black" alt="" height="155px", width="325px">
                </div>
                <div style="flex:2;">
                <div style="text-align:left; margin-bottom:10px;">
                    <h3 style="text-align:left;color:black;">Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication</h3>
                </div>
                <div style="line-height: 1.5;">
                    <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ"><b>Jinlong Li</b></a>,
                    <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,  
                    <a href="">Xinyu Liu</a>,
                    <a href="">Jin Ma</a>,
                    <a href="">Zicheng Chi</a>,
                    <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>,     
                    <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>
                    <br>
                    <b>Trans on Intelligent Vehicles 2023</b>
                    <a href="./files/TIV_2023_V2V_CDL.pdf" target="_blank">[Paper]</a>
                    <a href="https://github.com/jinlong17/V2VLC" target="_blank"> [Code]</a> 
                    <br>
                    "This work proposes the first research on V2V cooperative perception under lossy communication."</b>
                </div>
                </div>
            </div>


            <div style="height: 20px;"></div>
            <div style="height: 20px;"></div>

            <div style="display:flex; align-items:center;">
                <div style="flex:1; padding-right: 10px;">
                <img class="img-fluid img-rounded" src="./figure/v2v4real.png" style="border:1px solid black" alt="" height="165px", width="325px">
                </div>
                <div style="flex:2;">
                <div style="text-align:left; margin-bottom:10px;">
                    <h3 style="text-align:left;color:black;">V2V4Real:  A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception</h3>
                </div>
                <div style="line-height: 1.5;">
                    <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                    <a href="" target="_blank">Xin Xia</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ" target="_blank"><b>Jinlong Li</b></a>,
                    <a href="" target="_blank">Hanzhao Li</a>,
                    <a href="" target="_blank">Shuo Zhang</a>,
                    <a href="https://scholar.google.com/citations?user=9ajdZaEAAAAJ&hl=zh-TW">Zhengzhong Tu</a>,
                    <a href="">Zonglin Meng</a>,
                    <a href="https://scholar.google.com/citations?user=04j4RzkAAAAJ&hl">Hao Xiang</a>,
                    <a href="https://www.linkedin.com/in/xiaoyu%EF%BC%88cheri%EF%BC%89-dong-21b50095/">Xiaoyu Dong</a>, 
                    <a href="">Rui Song</a>, 
                    <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>,
                    <a href="https://scholar.google.com/citations?user=9D4aG8AAAAAJ&hl=en">Bolei Zhou</a>,
                    <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>  
                    <br> 
                    <b>CVPR 2023 <font color="red">Highlight (2.5% of 9155 submissions)</font> </b>
                    <a href="" target="_blank">[Project]</a>
                    <a href="https://arxiv.org/abs/2303.07601" target="_blank"> [Paper]</a>
                    <a href="https://github.com/ucla-mobility/V2V4Real" target="_blank">[Code]</a>
                    <br>
                    "This work presents the first large-scale real-world dataset for V2V perception."
                     <!-- V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes.<b>It was selected as Highlight (2.5% of 9155 submissions) at CVPR 2023. -->

                     </b>
                </div>
                </div>
            </div>

            
            <div style="height: 20px;"></div>
            <div style="height: 20px;"></div>
            
            <div style="display:flex; align-items:center;">
                <div style="flex:1; padding-right: 10px;">
                <img class="img-fluid img-rounded" src="./figure/domain_daption.png" style="border:1px solid black" alt="" height="155px", width="325px">
                </div>
                <div style="flex:2;">
                <div style="text-align:left; margin-bottom:10px;">
                    <h3 style="text-align:left;color:black;">Bridging the domain gap for multi-agent perception</h3>
                </div>
                <div style="line-height: 2;">
                    <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank">Runsheng Xu</a>,
                    <a href="https://scholar.google.com/citations?hl=en&user=tYIQiHgAAAAJ"><b>Jinlong Li</b></a>,
                    <a href="https://www.linkedin.com/in/xiaoyu%EF%BC%88cheri%EF%BC%89-dong-21b50095/">Xiaoyu Dong</a>,     
                    <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>,
                    <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>
                </b>
                    <br>
                    <b>ICRA 2023</b>
                    <!-- <a href="https://arxiv.org/pdf/2210.08451.pdf" target="_blank">[Paper]</a> -->
                    <a href="https://arxiv.org/pdf/2210.08451.pdf" target="_blank">[Paper]</a>
                    <a href="https://github.com/DerrickXuNu/MPDA" target="_blank"> [Code]</a> 
                    <br>
                    <!-- Existing multi-agent perception algorithms assume all agents have identical CNN, which might not be practical in the real world. The transmitted features can have a large domain gap when the models differ, leading to a dramatic performance drop in multi-agent perception. This paper proposes the first lightweight framework to bridge such domain gaps for multi-agent perception, which can be a plug-in module for most of the existing systems. -->
                    "The first work to bridge the domain gap for multi-agent perception."
                    <br>
                </div>
                </div>
            </div>


    
    </div>
    </section> 


<A NAME="Publications"><h2>Publications</h2></A>
<ul>
 
<p><b>Conferences</b>: </p>
<font size="3"> 
<ul>
 
<p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span>[9] <b>Jinlong Li</b>, Baolu Li, Zhengzhong Tu, Xinyu Liu,  Qing Guo, Felix Juefei-Xu, Runsheng Xu, Hongkai Yu. Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving[C].
    <i> Proceedings of  IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR2024)</i>, 
    2024.
    [<a href= "" target="_blank">Link</a>] 
    [<a href= "" target="_blank">PDF</a>]
    [<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
    [<a href="" target="_blank">Code</a>]
    </span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span>[8] <b>Jinlong Li</b>, Baolu Li, Xinyu Liu, Runsheng Xu, Jiaqi Ma, Hongkai Yu. Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources[C].
    <i> International Conference on Robotics and Automation (ICRA2024)</i>, 
    2024.
    [<a href= "https://arxiv.org/abs/2402.04273" target="_blank">Link</a>] 
    [<a href= "https://arxiv.org/pdf/2402.04273.pdf" target="_blank">PDF</a>]
    [<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
    [<a href="" target="_blank">Code</a>]
    </span>
</p>



<p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span>[7] <b>Jinlong Li</b>, Baolu Li, Xinyu Liu, Jianwu Fang, Felix Juefei-Xu, Qing Guo, Hongkai Yu. AdvGPS: Adversarial GPS for Multi-Agent Perception Attack[C].
    <i> International Conference on Robotics and Automation (ICRA2024)</i>, 
    2024.
    [<a href= "https://arxiv.org/abs/2401.17499" target="_blank">Link</a>] 
    [<a href= "https://arxiv.org/pdf/2401.17499.pdf" target="_blank">PDF</a>]
    [<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
    [<a href="" target="_blank">Code</a>]
    </span>
</p>



<p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span>[6] <b>Jinlong Li</b>, Runsheng Xu, Xinyu Liu, Baolu Li, Qin Zou, Jiaqi Ma, Hongkai Yu. S2R-ViT for multi-agent cooperative perception: Bridging the gap from simulation to reality[C].
    <i> International Conference on Robotics and Automation (ICRA2024)</i>, 
    2024.
    [<a href= "https://arxiv.org/abs/2307.07935" target="_blank">Link</a>] 
    [<a href= "https://arxiv.org/pdf/2307.07935.pdf" target="_blank">PDF</a>]
    [<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
    [<a href="" target="_blank">Code</a>]
    </span>
</p>



<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[5] Runsheng Xu, <b>Jinlong Li</b>, Xiaoyu Dong, Hongkai Yu, Jiaqi Ma. Bridging the domain gap for multi-agent perception[C].
<i> International Conference on Robotics and Automation (ICRA2023)</i>, 
2023.
[<a href= "https://arxiv.org/abs/2210.08451" target="_blank">Link</a>] 
[<a href= "https://arxiv.org/pdf/2210.08451.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/DerrickXuNu/MPDA" target="_blank">Code</a>]
[<a href="https://github.com/DerrickXuNu/MPDA" target="_blank">Dataset</a>] 
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[4] Runsheng Xu, Xin Xia,<b>Jinlong Li</b>, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, Jiaqi Ma. V2V4Real: A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception[C].
<i> Proceedings of  IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR2023)</i>, 
2023.
[<a href= "https://arxiv.org/abs/2303.07601" target="_blank">Link</a>] 
[<a href= "https://arxiv.org/pdf/2303.07601.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/ucla-mobility/V2V4Real" target="_blank">Code</a>]
[<a href="https://mobility-lab.seas.ucla.edu/v2v4real/" target="_blank">Dataset</a>] 
</span>
</p>


 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[3] <b>Jinlong Li</b>, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, and Hongkai Yu. Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather[C].
<i> Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV2023)</i>, 
2023.
[<a href= "https://arxiv.org/abs/2210.15176" target="_blank">Link</a>] 
[<a href= "./files/WACV2023_DA_detection_round2.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/jinlong17/DA-Detect" target="_blank">Code</a>]
[<a href="https://github.com/jinlong17/DA-Detect" target="_blank">Dataset</a>] 
</span>
</p>
 
  <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[2] Runsheng Xu, Zhengzhong Tu, Yuanqi Du, Xiaoyu Dong, <b>Jinlong Li</b>, Zibo Meng, Jiaqi Ma, Alan Bovik, and Hongkai Yu. Pik-Fix: Restoring and Colorizing Old Photo[C].
<i> Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV2023)</i>, 
2023.
[<a href= "https://arxiv.org/abs/2205.01902" target="_blank">Link</a>] 
[<a href= "./files/2205.01902.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/DerrickXuNu/Pik-Fix" target="_blank">Code</a>]
[<a href="https://github.com/DerrickXuNu/Pik-Fix" target="_blank">Dataset</a>] 
</span>
</p>
 
 
 <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[1] Xu, Runsheng, Hao Xiang, Xin Xia, Xu Han, <b>Jinlong Li</b>, and Jiaqi Ma. OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication[C].
<i> IEEE international conference on robotics and automation (ICRA2022)</i>, 
2022.
[<a href= "https://arxiv.org/pdf/2109.07644.pdf" target="_blank">Link</a>] 
[<a href= "./files/2109.07644.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/DerrickXuNu/OpenCOOD" target="_blank">Code</a>]
[<a href="https://mobility-lab.seas.ucla.edu/opv2v/" target="_blank">Dataset</a>] 
</span>
</p>
</ul>

 

 
<p><b>Journals</b>: </p>
<font size="3"> 
<ul>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span>[9] <b>Jinlong Li</b>, Runsheng Xu, Xinyu Liu, Jin Ma, Baolu Li, Qin Zou, Jiaqi Ma, and Hongkai Yu. Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather[J].
    <i>IEEE Transactions on Intelligent Vehicles</i>, 
    2024.
    [<a href= "https://ieeexplore.ieee.org/document/10574400" target="_blank">Link</a>] 
    [<a href= "https://arxiv.org/abs/2307.09676" target="_blank">PDF</a>]
    [<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
    [<a href="https://github.com/jinlong17/DA-Detect" target="_blank">Code</a>]
    [<a href="https://github.com/jinlong17/DA-Detect" target="_blank">Dataset</a>] 
    </span>
    </p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[8] Xinyu Liu, <b>Jinlong Li</b>, Jin Ma, Huiming Sun, Zhigang Xu, Tianyun Zhang, Hongkai Yu. Deep transfer learning for intelligent vehicle perception: A survey[J].
<i>Green Energy and Intelligent Transportation</i>, 
2023:100125.
<!-- (<b>SCI Q1 IF = 5.009</b>) -->
[<a href= "https://www.sciencedirect.com/science/article/pii/S2773153723000610" target="_blank">Link</a>] 
[<a href= "https://arxiv.org/pdf/2306.15110.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
<!-- [<a href="https://github.com/jinlong17/V2VLC" target="_blank">Code</a>] -->
<!-- [<a href="https://github.com/jinlong17/V2VLC" target="_blank">Dataset</a>]  -->
</span>
</p>



<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[7] Talha Azfar, <b>Jinlong Li</b>, Hongkai Yu, Ruey L Cheu, Yisheng Lv, Ruimin Ke. Deep Learning-Based Computer Vision Methods for Complex Traffic Environments Perception: A Review[J].
<i>Data Science for Transportation</i>, 
2024.
<!-- (<b>SCI Q1 IF = 5.009</b>) -->
[<a href= "https://link.springer.com/article/10.1007/s42421-023-00086-7#citeas" target="_blank">Link</a>] 
[<a href= "https://link.springer.com/article/10.1007/s42421-023-00086-7#citeas" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
<!-- [<a href="https://github.com/jinlong17/V2VLC" target="_blank">Code</a>] -->
<!-- [<a href="https://github.com/jinlong17/V2VLC" target="_blank">Dataset</a>]  -->
</span>
</p>
    


<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[6] <b>Jinlong Li</b>, Runsheng Xu, Xinyu Liu, Jin Ma, Zicheng Chi, Jiaqi Ma, Hongkai Yu. Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy Communication[J].
<i>IEEE Transactions on Intelligent Vehicles</i>, 
2023.
<!-- (<b>SCI Q1 IF = 5.009</b>) -->
[<a href= "https://ieeexplore.ieee.org/document/10077757" target="_blank">Link</a>] 
[<a href= "./files/TIV_2023_V2V_CDL.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/jinlong17/V2VLC" target="_blank">Code</a>]
[<a href="https://github.com/jinlong17/V2VLC" target="_blank">Dataset</a>] 
</span>
</p>


<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[5] Lan Fu, Hongkai Yu, Felix Juefei-Xu, <b>Jinlong Li</b>, Qing Guo, Song Wang. Let There be Light: Improved Traffic Surveillance via Detail Preserving Night-to-Day Transfer[J].
<i>IEEE Transactions on Circuits and Systems for Video Technology</i>, 
2021.
<!-- (<b>SCI Q1 IF = 4.685</b>) -->
[<a href= "https://ieeexplore.ieee.org/document/9435348" target="_blank">Link</a>] 
[<a href= "./files/2105.05011.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://github.com/fl82hope/N2D-transfer" target="_blank">Code</a>]
[<a href="https://drive.google.com/drive/folders/1GQYFej223oaaXycFpYryVM4TrNUIwvZ9?usp=sharing" target="_blank">Dataset</a>] 
</span>
</p>
 

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[4] Ying Gao, <b>Jinlong Li</b>, Zhigang Xu, Zhangqi Liu, Xiangmo Zhao, Jianhua Chen. A novel image-based convolutional neural network approach for traffic congestion estimation[J].
<i>Expert Systems with Applications</i>, 
2021:115037.
<!-- (<b>SCI Q1 IF= 6.954, the co-first author</b>) -->
[<a href= "https://www.sciencedirect.com/science/article/abs/pii/S0957417421004784" target="_blank">Link</a>] 
[<a href= "./files/traffic congestion estimation.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]  
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[3] <b>Jinlong Li</b>, Zhigang Xu, Lan Fu, Xuesong Zhou, Hongkai Yu. Domain adaptation from daytime to nighttime: A situation-sensitive vehicle detection and traffic flow parameter estimation framework[J].
<i>Transportation Research Part C: Emerging Technologies</i>, 
2021, 124: 102946.
<!-- (<b>SCI Q1 IF= 8.089</b>) -->
[<a href= "https://www.sciencedirect.com/science/article/abs/pii/S0968090X20308433" target="_blank">Link</a>] 
[<a href= "./files/partC2021.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]
[<a href="https://drive.google.com/drive/folders/1GQYFej223oaaXycFpYryVM4TrNUIwvZ9?usp=sharing" target="_blank">Dataset</a>]   
</span>
</p>


<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[2] Zhigang Xu, <b>Jinlong Li</b>, et al. A Review on Intelligent Road and Its Related Key Technologies [J].
<i>China Journal of Highway and Transport</i>, 
2019, 32(8): 1-24.
<!-- (<b>EI Index</b>) -->
[<a href= "http://zgglxb.chd.edu.cn/CN/abstract/abstract3217.shtml" target="_blank">Link</a>] 
[<a href= "./files/2.智能公路发展现状与关键技术_徐志刚.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]  
</span>
</p>

<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[1] Zhigang Xu,Yanli Che, <b>Jinlong Li</b>, et al. Research progress on automatic image processing technology for pavement distress[J].
<i>Journal of Traffic and Transportation Engineering</i>, 
2019,19(01):172-190.
<!-- (<b>EI Index</b>) -->
[<a href= "http://transport.chd.edu.cn/article/id/201901017" target="_blank">Link</a>] 
[<a href= "./files/1.路面破损图像自动处理技术研究进展_徐志刚.pdf" target="_blank">PDF</a>]
[<a href="./files/BibTex.html" target="_blank">BibTeX</a>]  
</span>
</p>
</ul>
</font>
<br />
<br />
<br />

<!-- </section>
<section id="Publications" class="home-section">
     <div class="row">

            <div class="col-xs-12 col-md-6 section-heading text-right">
                <h1 style="font-size: 1.5em; font-weight: bold;">Selected Works</h1>
            </div>
                <div class="col-xs-12 col-md-6 section-heading text-right">
                    <h2 style="font-size: 1.5em; font-weight: bold; text-align:center">2023</h2>
                </div>
                    </div>
                        <div class="row">
                            <div class="col-md-3">
                                <img class="img-fluid img-rounded" src="./figure/v2v4real.png" style="border:1px solid black" alt="", height="195px">
                            </div>
                        <div class="col-md-9">
                            <b><font color="black">V2V4Real: A large-scale real-world dataset for Vehicle-to-Vehicle Cooperative Perception</font></b><br>
                            <a href="https://scholar.google.com/citations?user=QW6Ro8IAAAAJ&hl" target="_blank"><b>Runsheng Xu</b></a>,
                            <a href="" target="_blank">Xin Xia</a>,
                            <a href="" target="_blank">Jinlong Li</a>,
                            <a href="" target="_blank">Hanzhao Li</a>,
                            <a href="" target="_blank">Shuo Zhang</a>,
                            <a href="https://scholar.google.com/citations?user=9ajdZaEAAAAJ&hl=zh-TW">Zhengzhong Tu</a>,
                            <a href="">Zonglin Meng</a>,
                            <a href="https://scholar.google.com/citations?user=04j4RzkAAAAJ&hl">Hao Xiang</a>,
                            <a href="https://www.linkedin.com/in/xiaoyu%EF%BC%88cheri%EF%BC%89-dong-21b50095/">Xiaoyu Dong</a>, 
                            <a href="">Rui Song</a>, 
                            <a href="https://scholar.google.com/citations?user=JnQts0kAAAAJ&hl=en">Hongkai Yu</a>,
                            <a href="https://scholar.google.com/citations?user=9D4aG8AAAAAJ&hl=en">Bolei Zhou</a>,
                            <a href="https://scholar.google.com/citations?user=S3cQz1AAAAAJ&hl=en" target="_blank">Jiaqi Ma</a>
                            <br>
                            <b>CVPR 2023 
                            <a href="" target="_blank"> <small>[Project]</small></a>
                            <a href="https://arxiv.org/abs/2303.07601" target="_blank"> <small>[Paper]</small></a>
                            <a href="https://github.com/ucla-mobility/V2V4Real" target="_blank"> <small>[Code]</small></a> <br></b>

                        </div>
    </div><hr>
</section> -->
  



 
<!-- <p><b>Conferences</b>: </p> -->
<!-- <font size="3">  -->
<!-- <ul> -->

<!-- <p style="text-indent: -1.6rem;margin-left: 0rem;">
<span>[4] <b>Q. Zhang</b>, F. Sun, Q. Yuan, and L. Zhang, 
“Thick cloud removal for Sentinel-2 time-series images via combining deep prior and low-rank tensor completion,” 
<i>Proceeding of the IEEE International Geoscience and Remote Sensing Symposium (<b>IGARSS</b>)</i>, 
in Brussels, Belgium, pp. 2675-2678, 2021. 
(<b>EI, <font color="#FF0000">Oral</font></b>) 
[<a href="./files/Qiang Zhang_Slides_IGARSS2019.pdf" target="_blank">Slides</a>]
</span>
</ul>
</p> -->
 
<!-- </ul> -->

 
 

<!-- <A NAME="Projects"><h2>Projects</h2></A> -->
<!-- <font size="3">  -->
<!-- <ul> -->
<!-- <li><a href= "https://qzhang95.github.io/Projects/Global-Daily-Seamless-AMSR2/" target="_blank">SGD-SM V1.0</a></li> -->
<!-- </ul> -->
<!-- </font> -->
<!-- <br /> -->

<!-- </ul> -->


 
<A NAME="Services"><h2>Services</h2></A>

<!-- <p><b>Conference Organization</b>: </p>
<font size="3"> 
<ul> -->


<p><b>Academic Activities</b>: </p>
<font size="3"> 
<ul>
<li>Received the CSU Outstanding Achievement Award for Doctoral Research in Natural Sciences, Engineering. Spring 2024. [<a href="https://www.csuohio.edu/gsrc/grad-student-award-opportunities" target="_blank">Link</a>] [<a href="./files/Li O Doctoral Research award ltr 2023-24.pdf" target="_blank">File</a>]</li>
     
<li>Being a guest lecturer&Teaching Assistant and  of CIS694/EEC693/CIS593 Deep Learning in Cleveland State University, Spring 2024.</li>
<li>Invited talk for Cleveland State University & NSF MRI High Performance Computing Workshop, Fall 2023. [<a href="https://cis.csuohio.edu/~h.yu/mri.html" target="_blank">Link</a>]</li>
<li>Being a guest speaker of CIS694/EEC693/CIS593 Deep Learning in Cleveland State University, Spring 2023.</li>
<li>Being a Judge for Choose Ohio First Symposium at Cleveland State University, in Spring 2022.</li>
<li>The master's thesis "Research of Urban Road Vehicle Trajectory Tracking and Automatic Generation Algorithm Based on Deep Learning" was selected as an excellent master's thesis of CHTS(CHINA HIGHWAY AND TRANSPORTATION SOCIETY) in 2022.[<a href="https://it.chd.edu.cn/2022/0907/c7383a222878/page.htm" target="_blank">Link</a>] [<a href="./files/2022106.pdf" target="_blank">File</a>] </li>
 

 
<!-- <li>IGARSS 2021, <b>Session Chair</b> (Session: <a href= "https://igarss2021.com/view_session.php?SessionID=1105" target="_blank">Image Restoration</a>)</li> -->
</ul>
</font>
<br />
<br />
<br />
 
<!-- <p><b>Membership</b>: </p>
<font size="3"> 
<ul>
<li>IEEE, Student Member, 2018-Now</li>
<li>IEEE Geoscience and Remote Sensing Society (GRSS), Student Member, 2018-Now</li>
</ul>
</font>
<br />
<br />
<br /> -->
 
<p><b>Journal Reviewer</b>: </p>
<font size="3"> 
<ul>
<li>Expert Systems with Applications (<b>ESWA</b>)</li>
<li>IEEE Transactions on Intelligent Transportation Systems (<b>T-ITS</b>)</li>
<li>IEEE Transactions on Intelligent Vehicles  (<b>T-IV</b>)</li>
<li>IEEE Transactions on Vehicular Technology  (<b>T-VT</b>)</li>
<li>IEEE Transactions on Multimedia (<b>T-MM</b>)</li>
<li>IEEE Transactions on Circuits and Systems for Video Technology (<b>T-CSVT</b>)</li>
<li>IEEE Robotics and Automation Letters (<b>RA-L</b>)</li>
<li>IEEE Sensor Journal (<b>SJ</b>)</li>
<li>Neural Networks (<b>NN</b>)</li>
<li>IET Image Processing (<b>IET IP</b>)</li>
<li>IEEE Vehicular Technology Magazine (<b>VTM</b>)</li>
<li>International Journal of Machine Learning and Cybernetics (<b>IJMLC</b>)</li>

</ul>
</font>
<br />


<p><b>Conference Reviewer</b>: </p>
<font size="3"> 
<ul>
<!-- <li>European Conference on Computer Vision 2024 (<b>ECCV</b>)</li> -->
<li>IEEE International Conference on Robotics and Automation, 2024 (<b>ICRA</b>)</li>
<li>IEEE/CVF Winter Conference on Applications of Computer Vision, 2023 (<b>WACV</b>)</li>
<li>IEEE/CVF International Conference on Computer Vision,  2023 (<b>ICCV</b>)</li>
<li>European Conference on Computer Vision,  2024 (<b>ECCV</b>)</li>
<li>International Conference on Multimedia Information Processing and Retrieval, 2022, 2023 (<b>MIPR</b>)</li>
<li>IEEE/RSJ International Conference on Intelligent Robots and Systems, 2024 (<b>IROS</b>)</li>

</ul>
</font>
<br />
</ul>





 
<br />
<br />


<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5x3ebj080sx&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
 

<div id="article"></div>
<div id="back_top">
<div class="arrow"></div>
<div class="stick"></div>
</div>

<script>
$(function(){
    $(window).scroll(function(){  //If scroll
        var scrollt = document.documentElement.scrollTop + document.body.scrollTop; //Getting Height after scroll
        if( scrollt >400 )
        {  
            $("#back_top").fadeIn(400); 
        }
        else
        {
            $("#back_top").stop().fadeOut(400);
        }
    });

    $("#back_top").click(function(){ 

        $("html,body").animate({scrollTop:"0px"}, 200);

    }); 

});
</script>


<!--
All Rights Reserved by Jinlong Li. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

<!--
<font size="2"; color="#A0A0A0";>
<p style="text-align:center">Updating time: 2021.10.11</p>
</font>
-->

</body>
</html>
